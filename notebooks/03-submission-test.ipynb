{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Submission Notebook for CMI Kaggle Competition.\n",
    "This notebook integrates the trained model and the finalized feature engineering\n",
    "function within the required Kaggle API loop structure.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  kaggle_evaluation.cmi_gateway import CMIGateway\n",
    "import os, sys\n",
    "import pickle\n",
    "import joblib\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import PROJECT_PATH#, DATA_PATH\n",
    "from src.feature_engineering import create_wave1_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'  # Path to the directory containing test files\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "TEST_DEMOGRAPHICS_FILE = 'test_demographics.csv'\n",
    "\n",
    "required_files = [TEST_DATA_FILE, TEST_DEMOGRAPHICS_FILE]\n",
    "for file in required_files:\n",
    "    if not os.path.exists(os.path.join(DATA_PATH, file)):\n",
    "        raise FileNotFoundError(f\"Required test file not found: {os.path.join(DATA_PATH, file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a98a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Update these paths to where you save your final model and feature function ---\n",
    "MODEL_PATH = 'models_rev/wave1-catboost-best.cbm'\n",
    "FEATURE_FUNCTION_NAME = 'create_wave1_features' # Name of the final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL MAPS\n",
    "# Load training data\n",
    "df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "df_train_demos = pd.read_csv(os.path.join(DATA_PATH, 'train_demographics.csv'))\n",
    "\n",
    "# --- Create Helper Mappings for Evaluation Metric ---\n",
    "# Important for the custom F1 score function\n",
    "metadata = df_train[['gesture', 'sequence_type']].drop_duplicates()\n",
    "\n",
    "# Map gesture string to sequence type (Target vs. Non-Target)\n",
    "gesture_to_seq_type_map = metadata.set_index('gesture')['sequence_type'].to_dict()\n",
    "\n",
    "# Map gesture string to integer code and back\n",
    "gesture_map = {label: i for i, label in enumerate(metadata['gesture'].unique())}\n",
    "inv_gesture_map = {i: label for label, i in gesture_map.items()}\n",
    "\n",
    "# Validate\n",
    "print(f\"Gesture Map: {gesture_map}\")\n",
    "print(f\"\\nInverted Gesture Map: {inv_gesture_map}\")\n",
    "print(f\"\\nGesture To Sequence Type: {gesture_to_seq_type_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0550b6",
   "metadata": {},
   "source": [
    "## Load model and feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cat.CatBoostClassifier()\n",
    "model.load_model(MODEL_PATH);\n",
    "print(f\"Model parameters: {model.get_all_params()}\")\n",
    "print(f\"\\nModel classes: {model.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_submission_features(single_sequence_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Final feature engineering function for submission using Wave 1 logic.\n",
    "    This function:\n",
    "    1. Takes a single pandas DataFrame (e.g., `test_sequence_df` from the API).\n",
    "    2. Calls the modular `create_wave1_features` function.\n",
    "    \n",
    "    Note: create_wave1_features processes one sequence_id but expects a df with 'sequence_id'.\n",
    "    The API provides data for one sequence_id, so we need to ensure it's handled correctly.\n",
    "    The function groups by 'sequence_id', so it should work on the slice provided by the API.\n",
    "    \n",
    "    3. Post-processes the result to return a single-row DataFrame of features.\n",
    "    4. Ensures 'sequence_id' column is present for later dropping.\n",
    "    5. Handles -1.0 in ToF columns correctly (should be handled inside `create_wave1_features`).\n",
    "    \"\"\"\n",
    "    print(f\"  [Feature Eng.] Processing sequence data of shape {single_sequence_df.shape}...\")\n",
    "    \n",
    "    # --- Proactive State Management: Validate Input ---\n",
    "    required_base_cols = ['sequence_id', 'phase', 'subject', 'acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_w', 'rot_x', 'rot_y', 'rot_z', 'thm_1', 'thm_2',\n",
    "                          'thm_3', 'thm_4', 'thm_5', 'sequence_counter']\n",
    "    # Check for ToF columns\n",
    "    tof_cols_exist = any(col.startswith('tof_') for col in single_sequence_df.columns)\n",
    "    if not tof_cols_exist:\n",
    "        raise ValueError(\"No ToF columns (starting with 'tof_') found in API-provided data.\")\n",
    "    missing_base_cols = [col for col in required_base_cols if col not in single_sequence_df.columns]\n",
    "    if missing_base_cols:\n",
    "        raise ValueError(f\"Missing required base columns for Wave 1 feature engineering: {missing_base_cols}\")\n",
    "    \n",
    "    # Call feature engineering function from src.feature_engineering\n",
    "    # The function will group by 'sequence_id' to produce one output row.abs\n",
    "    try:\n",
    "        # This function is expected to handle -1.0 in ToF internally.\n",
    "        features_df = create_wave1_features(single_sequence_df)\n",
    "        print(f\"  [Feature Eng.] FE function returned shape: {features_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [Feature Eng.] Error in modular function: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "    # Post processing - ensure the output is a single row DF\n",
    "    if not isinstance(features_df, pd.DataFrame):\n",
    "        raise  TypeError(\"Modular feature function must return a pandas Dataframe!\")\n",
    "    if len(features_df) != 1:\n",
    "        raise ValueError(f\"Modular feature function should return 1 row, got {len(features_df)}.\")\n",
    "    \n",
    "    \n",
    "    # Ensure sequence_id is present (will be dropped later)\n",
    "    if 'sequence_id' not in features_df.columns:\n",
    "        # Get sequence_id from the input data (assuming it is consistent)\n",
    "        seq_id_from_input = single_sequence_df['sequence_id'].iloc[0]\n",
    "        features_df['sequence_id'] = seq_id_from_input\n",
    "        print(f\"  [Feature Eng.] Added missing 'sequence_id' column with value {seq_id_from_input}.\")\n",
    "        \n",
    "    print(f\"  [Feature Eng.] Final features ready. Shape: {features_df.shape}\")\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02806f",
   "metadata": {},
   "source": [
    "## Submission loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Main Submission Loop (Revised for CMIGateway - Ensuring Path Unpacking with Tuple) ---\n",
    "\"\"\"\n",
    "The main Kaggle submission loop logic.\n",
    "This cell demonstrates how to interact with CMIGateway for testing.\n",
    "Includes enhanced debugging for data loading issues.\n",
    "\"\"\"\n",
    "\n",
    "# --- Define the prediction function (CORRECTED IMPLEMENTATION) ---\n",
    "def predict_single_sequence_for_submission(model, single_sequence_df: pd.DataFrame, feature_func, inv_map: dict) -> str:\n",
    "    \"\"\"\n",
    "    Performs feature engineering and prediction for a single sequence DataFrame for the final submission.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"      Feature engineering for sequence with {len(single_sequence_df)} rows...\")\n",
    "        features_df = feature_func(single_sequence_df)\n",
    "        print(f\"      Feature engineering complete. Output shape: {features_df.shape}\")\n",
    "\n",
    "        # --- Proactive Debugging: Check feature names ---\n",
    "        # Get the feature names the model was trained on\n",
    "        # Handle potential differences in how feature names are stored\n",
    "        if hasattr(model, 'feature_names_') and model.feature_names_ is not None:\n",
    "            expected_feature_names = list(model.feature_names_)\n",
    "        elif hasattr(model, 'get_feature_names') and callable(getattr(model, 'get_feature_names')):\n",
    "             # Some models might have a get_feature_names method\n",
    "             expected_feature_names = list(model.get_feature_names())\n",
    "        else:\n",
    "             # Fallback: This is risky, assumes features_df columns are correct\n",
    "             # This should ideally not be reached.\n",
    "             print(\"      WARNING: Could not determine model's expected feature names from model object attributes.\")\n",
    "             expected_feature_names = list(features_df.columns)\n",
    "             # Remove non-feature columns if they exist in features_df\n",
    "             non_feature_cols = ['sequence_id', 'subject', 'gesture', 'gesture_encoded'] # Common ones\n",
    "             expected_feature_names = [f for f in expected_feature_names if f not in non_feature_cols]\n",
    "\n",
    "        print(f\"      Model expects {len(expected_feature_names)} features.\")\n",
    "\n",
    "        # Ensure all expected features are present\n",
    "        available_features = set(features_df.columns)\n",
    "        missing_features = set(expected_feature_names) - available_features\n",
    "\n",
    "        if missing_features:\n",
    "            print(f\"      ERROR: Missing features for model: {missing_features}\")\n",
    "            print(f\"      First few missing: {list(missing_features)[:5]}\")\n",
    "            raise ValueError(f\"Feature mismatch: Missing features {missing_features}\")\n",
    "\n",
    "        # Select only the features the model expects\n",
    "        model_input_features = features_df[expected_feature_names]\n",
    "        \n",
    "        # Ensure it's a single row if the model expects it (common for sequence-level models)\n",
    "        # The feature function should ideally produce one row per sequence.\n",
    "        if model_input_features.shape[0] != 1:\n",
    "             print(f\"      WARNING: Feature function returned {model_input_features.shape[0]} rows. Expected 1. Using first row.\")\n",
    "             # This slicing keeps it as a DataFrame\n",
    "             model_input_features = model_input_features.iloc[[0]] # Use double brackets for iloc\n",
    "\n",
    "        print(f\"      Features selected for model input. Shape: {model_input_features.shape}\")\n",
    "\n",
    "        # --- Model Prediction ---\n",
    "        print(\"      Calling model.predict_proba...\")\n",
    "        y_pred_proba = model.predict_proba(model_input_features)\n",
    "        print(f\"      Model prediction complete. Proba shape: {y_pred_proba.shape}\")\n",
    "        predicted_class_index = np.argmax(y_pred_proba, axis=1)[0] # Get index for first (only) row\n",
    "        predicted_gesture_string = inv_map[predicted_class_index]\n",
    "        print(f\"      Prediction made: '{predicted_gesture_string}'\")\n",
    "        return predicted_gesture_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      ERROR during feature engineering or prediction for sequence: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # --- Proactive Debugging: Fallback in Final Submission ---\n",
    "        # In the final submission, it's often better to predict a default gesture\n",
    "        # than to crash the entire process or return None.\n",
    "        default_gesture_index = 0 # Or choose based on training frequency\n",
    "        # Safely get a default gesture, fallback to first in map if index 0 doesn't exist\n",
    "        default_gesture = inv_map.get(default_gesture_index, list(inv_map.values())[0]) \n",
    "        print(f\"      Fallback prediction due to error: '{default_gesture}'\")\n",
    "        return default_gesture # Return fallback string, not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82111cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Main Submission Loop (Revised for CMIGateway - Ensuring Path Unpacking with Tuple) ---\n",
    "# ... (keep the predict_single_sequence_for_submission function as is) ...\n",
    "\n",
    "# --- Initialize the API (Corrected: Using Explicit Local Paths AND calling unpack_data_paths) ---\n",
    "print(\"\\nInitializing Kaggle API (CMIGateway) with explicit LOCAL data paths...\")\n",
    "try:\n",
    "    # --- Proactive State Management: Define and Verify Local Paths ---\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    local_data_base_dir = os.path.join(project_root, \"notebooks\", \"kaggle\", \"input\", \"cmi-detect-behavior-with-sensor-data\")\n",
    "\n",
    "    local_test_path = os.path.join(local_data_base_dir, \"test.csv\")\n",
    "    local_demo_path = os.path.join(local_data_base_dir, \"test_demographics.csv\")\n",
    "\n",
    "    local_test_path = os.path.abspath(os.path.expanduser(local_test_path))\n",
    "    local_demo_path = os.path.abspath(os.path.expanduser(local_demo_path))\n",
    "\n",
    "    print(f\"  Configured local test path: {local_test_path}\")\n",
    "    print(f\"  Configured local demo path: {local_demo_path}\")\n",
    "\n",
    "    # Check if files exist at the specified local paths BEFORE passing to gateway\n",
    "    if not os.path.exists(local_test_path):\n",
    "        raise FileNotFoundError(f\"Local test file not found at: {local_test_path}\")\n",
    "    if not os.path.exists(local_demo_path):\n",
    "        raise FileNotFoundError(f\"Local demographics file not found at: {local_demo_path}\")\n",
    "    print(\"  Local test files confirmed to exist.\")\n",
    "\n",
    "    # --- Pass the paths as a tuple ---\n",
    "    env = CMIGateway(data_paths=(local_test_path, local_demo_path))\n",
    "    print(\"API (CMIGateway) instantiated with explicit local paths tuple.\")\n",
    "\n",
    "    # --- CRITICAL: Explicitly call unpack_data_paths to set internal attributes ---\n",
    "    print(\"Calling unpack_data_paths() to set internal paths...\")\n",
    "    env.unpack_data_paths() # This should now correctly use self.data_paths[0] and [1]\n",
    "    print(\"unpack_data_paths() completed.\")\n",
    "\n",
    "    # --- Proactive Debugging: Verify Internal Paths are Set ---\n",
    "    if not hasattr(env, 'test_path') or not hasattr(env, 'demographics_path'):\n",
    "         raise AttributeError(\"CMIGateway failed to set test_path or demographics_path after unpack_data_paths().\")\n",
    "    print(f\"  Internal paths set by gateway: test_path='{env.test_path}', demographics_path='{env.demographics_path}'\")\n",
    "    # Optional: Check if they match expected\n",
    "    if env.test_path != local_test_path or env.demographics_path != local_demo_path:\n",
    "        print(\"  WARNING: Internal paths do not exactly match provided paths, but they should point to the same files.\")\n",
    "\n",
    "    # --- Enhanced Debugging: Check File Reading (using gateway's internal paths) ---\n",
    "    print(\"\\n--- Enhanced Debugging: Checking File Reading with Gateway's Paths ---\")\n",
    "    import polars as pl\n",
    "\n",
    "    print(\"Attempting to read test.csv with Polars (using gateway's internal path)...\")\n",
    "    try:\n",
    "        test_pl = pl.read_csv(env.test_path)\n",
    "        print(f\"  Successfully read test.csv. Shape: {test_pl.shape}\")\n",
    "        if test_pl.is_empty():\n",
    "            print(\"  WARNING: test.csv is empty!\")\n",
    "        else:\n",
    "             unique_seq_ids = test_pl['sequence_id'].unique()\n",
    "             print(f\"  Number of unique sequence_ids found: {len(unique_seq_ids)}\")\n",
    "    except Exception as read_error:\n",
    "        print(f\"  ERROR reading test.csv with Polars: {read_error}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Attempting to read test_demographics.csv with Polars (using gateway's internal path)...\")\n",
    "    try:\n",
    "        demos_pl = pl.read_csv(env.demographics_path)\n",
    "        print(f\"  Successfully read test_demographics.csv. Shape: {demos_pl.shape}\")\n",
    "        if demos_pl.is_empty():\n",
    "            print(\"  WARNING: test_demographics.csv is empty!\")\n",
    "    except Exception as read_error:\n",
    "        print(f\"  ERROR reading test_demographics.csv with Polars: {read_error}\")\n",
    "        raise\n",
    "    print(\"--- End Enhanced Debugging ---\\n\")\n",
    "\n",
    "    # --- Get the generator ---\n",
    "    data_generator = env.generate_data_batches()\n",
    "    print(\"Data generator created.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during API initialization or data check: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- The Submission Loop (Interacting with the generator) ---\n",
    "print(\"Entering main prediction loop (testing first few sequences)...\")\n",
    "sequence_count = 0\n",
    "MAX_SEQUENCES_TO_TEST = 5\n",
    "sequences_processed = 0 # New counter to track actual loop entries\n",
    "\n",
    "try:\n",
    "    for batch_data, row_ids_df in data_generator:\n",
    "        sequences_processed += 1 # Increment if loop body is entered\n",
    "        sequence_count += 1\n",
    "        if sequence_count > MAX_SEQUENCES_TO_TEST:\n",
    "            print(f\"Reached test limit of {MAX_SEQUENCES_TO_TEST} sequences. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        # --- Extract sequence_id and data ---\n",
    "        sequence_pl_df, sequence_demos_pl_df = batch_data\n",
    "        sequence_id_series = row_ids_df['sequence_id'] # This should be a Polars Series\n",
    "        sequence_id = sequence_id_series[0] # Get the single sequence ID for this batch\n",
    "        \n",
    "        print(f\"\\nProcessing sequence {sequence_count}: {sequence_id}\")\n",
    "        print(f\"  Sequence data shape (Polars): {sequence_pl_df.shape}\")\n",
    "        # print(f\"  Sequence columns: {sequence_pl_df.columns}\") # Uncomment for detailed inspection\n",
    "        print(f\"  Demographics data shape (Polars): {sequence_demos_pl_df.shape}\")\n",
    "\n",
    "        # --- Convert Polars DataFrames to Pandas ---\n",
    "        # The API serves data using Polars. Convert for our functions.\n",
    "        if hasattr(sequence_pl_df, 'to_pandas'):\n",
    "            test_sequence_df = sequence_pl_df.to_pandas()\n",
    "            # The gateway code merges demographics based on 'subject' internally.\n",
    "            # The resulting test_sequence_df should already contain demographic columns\n",
    "            # if the merge in generate_data_batches worked correctly.\n",
    "            # If you need to access demographics separately, you can convert sequence_demos_pl_df too.\n",
    "            # demos_df = sequence_demos_pl_df.to_pandas() \n",
    "        else:\n",
    "            print(f\"      ERROR: Expected Polars DataFrame from API.\")\n",
    "            continue # Skip this sequence\n",
    "\n",
    "        # --- Proactive Debugging: Check for train-only columns ---\n",
    "        # This check is important. If 'phase' is present, it's an error in the feature function usage.\n",
    "        if 'phase' in test_sequence_df.columns:\n",
    "             print(f\"      CRITICAL WARNING: 'phase' column found in test data served by API. This column is train-only. Feature engineering MUST NOT use it.\")\n",
    "\n",
    "        # --- Feature Engineering and Model Prediction ---\n",
    "        try:\n",
    "            predicted_gesture_string = predict_single_sequence_for_submission(\n",
    "                model=model,\n",
    "                single_sequence_df=test_sequence_df, # This is the full data for one sequence\n",
    "                feature_func=create_wave1_features, # <-- Ensure this function is designed for single-sequence input\n",
    "                inv_map=inv_gesture_map\n",
    "            )\n",
    "            print(f\"  Predicted gesture string: '{predicted_gesture_string}' (Type: {type(predicted_gesture_string)})\")\n",
    "            # --- Proactive Debugging: Ensure prediction is a string ---\n",
    "            if not isinstance(predicted_gesture_string, str):\n",
    "                print(f\"  CRITICAL ERROR: predict_single_sequence_for_submission returned a non-string: {predicted_gesture_string}\")\n",
    "                raise TypeError(f\"Prediction must be a string, got {type(predicted_gesture_string)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR in prediction pipeline for {sequence_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Fallback prediction (should be a string now)\n",
    "            default_gesture_index = 0\n",
    "            default_gesture = inv_gesture_map.get(default_gesture_index, list(inv_gesture_map.values())[0])\n",
    "            predicted_gesture_string = default_gesture\n",
    "            print(f\"  Using fallback prediction for {sequence_id}: '{predicted_gesture_string}'\")\n",
    "\n",
    "        # --- Submit Prediction ---\n",
    "        try:\n",
    "            print(f\"  Submitting prediction '{predicted_gesture_string}' for {sequence_id}...\")\n",
    "            # The gateway's predict method likely needs the sequence_id and the prediction string.\n",
    "            # The row_ids_df might also be required by the base class validation.\n",
    "            # Based on BaseGateway.validate_prediction_batch, it takes (prediction, row_ids)\n",
    "            # Let's try the standard predict call first. If it fails, we might need to pass row_ids_df.\n",
    "            \n",
    "            # --- Proactive Debugging: Check prediction validity before submission ---\n",
    "            # The gateway validates predictions. Let's do a quick check ourselves.\n",
    "            # Based on cmi_gateway.py, valid gestures are in env.all_gestures\n",
    "            if hasattr(env, 'all_gestures') and predicted_gesture_string not in env.all_gestures:\n",
    "                print(f\"  WARNING: Predicted gesture '{predicted_gesture_string}' not in gateway's known list. Submission might fail.\")\n",
    "            \n",
    "            # Try the primary signature based on BaseGateway validation logic\n",
    "            env.predict(predicted_gesture_string, row_ids_df) # Try passing row_ids_df as per base validation\n",
    "            # If the above fails, try: env.predict(sequence_id, predicted_gesture_string)\n",
    "            print(f\"  Prediction submitted for {sequence_id}.\")\n",
    "        except TypeError as te:\n",
    "            print(f\"  TypeError on env.predict (likely wrong signature): {te}\")\n",
    "            print(\"  Trying alternative submission method: env.predict(sequence_id, prediction)...\")\n",
    "            try:\n",
    "                # Fallback signature based on initial schema\n",
    "                env.predict(sequence_id, predicted_gesture_string)\n",
    "                print(f\"  Prediction submitted for {sequence_id} (using fallback signature).\")\n",
    "            except Exception as te2:\n",
    "                print(f\"  Fallback submission also failed: {te2}\")\n",
    "                raise te2 # Re-raise the second error\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR submitting prediction for {sequence_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise e # Consider if you want to stop on submission error or continue\n",
    "\n",
    "    print(f\"\\nPrediction loop completed successfully. Iterated through loop body {sequences_processed} times.\")\n",
    "    print(f\"Tested {min(sequence_count, MAX_SEQUENCES_TO_TEST)} sequences.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!! CRITICAL ERROR in main loop !!!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== Final Submission Notebook Execution (Test Loop) Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ef036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
